<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>FigStep的论文阅读</title>
    <url>/posts/3790135051/</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/12/01/hq9WksMYbzmgZe1.png" alt="image.png"></p>
<blockquote>
<p>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts论文学习</p>
</blockquote>
<span id="more"></span>
<h1 id="FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts"><a href="#FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts" class="headerlink" title="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"></a>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</h1><p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<ul>
<li><a href="https://github.com/ThuCCSLab/FigStep">代码</a></li>
<li><a href="https://arxiv.org/pdf/2311.05608">原文链接</a></li>
</ul>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<h4 id="1-1-安全性对齐"><a href="#1-1-安全性对齐" class="headerlink" title="1.1 安全性对齐"></a>1.1 安全性对齐</h4><p><strong>概念</strong> ：安全性对齐通常指的是确保模型的输出和行为与预期目标和社会规范相一致，不会产生有害或者不当的结果。</p>
<p><strong>分类</strong>：安全性对齐主要涉及以下几个方面：</p>
<ol>
<li><strong>伦理和道德对齐</strong>：确保模型的输出不违反伦理和道德规范。</li>
<li><strong>法律和法规对齐</strong>：确保模型的行为符合相关法律和法规的要求。</li>
<li><strong>用户意图对齐</strong>：确保模型的输出和用户的预期和需求一致，避免误导或者错误的信息。</li>
<li><strong>社会价值对齐</strong>：确保模型的行为和输出符合社会普遍接受的价值观和标准。</li>
</ol>
<h4 id="1-2-VLM"><a href="#1-2-VLM" class="headerlink" title="1.2 VLM"></a>1.2 VLM</h4><ol>
<li><strong>语言模块</strong>：是一个预先训练的LLM，大多数接受了安全对齐。</li>
<li><strong>视觉模块</strong>：是一个图像编码器，将图像转换成为视觉特征。</li>
<li><strong>连接模块</strong>：将视觉特征从视觉模块映射到与语言模块相同的嵌入空间。</li>
</ol>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文介绍了FigStep，一种简单有效的针对于VLM (vision-language models) 的越狱算法，方法侧重于将有害的文本指令转换为排版图像，以便能够绕过VLM中的安全对齐。最后通过ASR (平均攻击成功率) 的显示，得到了不错的效果。这表明了对VLM采用更加复杂的对齐方式，是显得非常重要的。</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<ul>
<li><p><strong>革命</strong>：最近大模型经历了一场大革命，因为虽然它们建立在LLM的基础之上，但可以包括其他模态 (如图片) 。</p>
</li>
<li><p><strong>安全</strong>：VLM的安全性缺乏系统的评估，而且建造者们还可能对他们自己模型的安全性有着盲目自信。</p>
</li>
<li><p><strong>直觉</strong>：本文提出FigStep，主要基于以下三个直觉</p>
<ul>
<li>VLM能够理解并遵循排版视觉模块的特征</li>
<li>局部上文本的安全对齐，可能无法保证整体上文本和图片的安全对齐</li>
<li>底层LLM的逐步推理能力可以提高越狱性能</li>
</ul>
</li>
</ul>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<ul>
<li><p><strong>文本模块安全的重要性</strong>：因为最终生成的结果，是由图像编码器转换为视觉特征，再由连接器映射到文本模块相同的嵌入空间，最终由文本模块响应生成，因此文本模块的安全性十分重要。</p>
</li>
<li><p><strong>有害内容形式转换</strong>：FigStep没有直接输入有害的文字指令，而是通过排版将有害的内容转换为图像。 这样做的目的是绕过VLM文本模块中的安全对齐，诱导其输出违反常见AI安全策略的不安全响应。</p>
<p><img src="https://i-blog.csdnimg.cn/img_convert/96dc544f34e8ec4d8606f030ffa086e6.png" alt=""></p>
<ul>
<li><p>具体实现步骤</p>
<ul>
<li><p>首先释义模块将有害问题转换为以“Steps to”, “List of”的一些陈述性语句。</p>
</li>
<li><p>然后使用排版将有害的内容嵌入到图像之中。</p>
</li>
<li><p>最后利用良性文本来激发VLM的推理能力，根据图像内容来生成详细响应。</p>
</li>
<li><p>除此之外还提出了FigStep Pro，用于越狱GRT-4V。两者之间的差别是FigStep Pro还包括一个额外的预处理步骤，用来绕过GPT-4V系统内的OCR检测器。</p>
<p><img src="https://i-blog.csdnimg.cn/img_convert/8611372104f3f51732eb28e9b77ea25a.png" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>参数设置</strong>：</p>
<ul>
<li>用GPT-4创建了一个名为SafeBench的安全基准，其中包含500个有害问题，涵盖了使用禁止政策的常见基准。<ul>
<li>selfBench安全基准的构建<ul>
<li>首先收集常见禁止主题，然后再选取10个作为安全基准</li>
<li>然后编写描述，再使用GPT-4来进行生成50个问题，并手动审查</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>广泛性</strong> ：在6个开源模型 (2个来自于LLaVA-v1.5, 3个来自于MiniGPT4, 1个CogVLM)，以及最先进的闭源模型GPT-4V证明了有效性</p>
</li>
<li><p><strong>baseline</strong>：直接向六个开源VLM提供纯文本的有害问题来进行基线评估，成功率为44.80%</p>
</li>
</ul>
<p><strong>评估设置</strong></p>
<ul>
<li><p>对禁止问题进行改写，改写之后输入VLM中，然后得到了模型生成的n个回答，然后进行判断模型输出是否属于不安全的内容，最后如果在所有问题之中只要有一次评估器输出为1，那么则就表明攻击成功</p>
<ul>
<li>针对每一个问题</li>
</ul>
<script type="math/tex; mode=display">\text{isSuccessJ}(Q^*) = \max_{i=1}^{n} E(Q^*, R_J(T^*), i) \in \{0, 1\}.</script><ul>
<li><p>所有问题</p>
<script type="math/tex; mode=display">\text{ASRJ}(D) = \frac{\sum_{Q^* \in D} \text{isSuccessJ}(Q^*)}{|D|}.</script></li>
</ul>
</li>
</ul>
<p><strong>消融实验</strong>：</p>
<ul>
<li>首先第一种查询，直接将改变后中的图片中的文本，直接输入，查看效果</li>
<li>第二种查询，将出现的所有文本信息输入，包括文本煽动性信息，进行查询 (目的是为了测试 (1) 通过释义将模型参与延续任务是否可以提高越狱的成功率 (2) 单一模态是否能够做到高ASR )</li>
<li>第三种查询只有图像 (用于探索FigStep中煽动性文本提示的作用)</li>
<li>第四种则是直接将SafeBench Tiny中的原始问题放置于图像中，而文本则是直接要求提供答案</li>
</ul>
<p><strong>FigStep生成</strong></p>
<ul>
<li>不同的生成方式 (字体的形态，颜色；背景的颜色) 对最后的结果都有影响</li>
</ul>
<p><strong>重复设置n的次数</strong></p>
<ul>
<li>探究查询次数设置，次数越多效果越好，但5次便能得到不错的效果</li>
</ul>
<p><strong>Temperature设置的探究</strong></p>
<ul>
<li>探究不同的Temperature下，ASR的效果，Temperature越高创造性越强，ASR越高</li>
</ul>
<p><strong>系统提示词的设置</strong></p>
<ul>
<li>通过实验，发现不同的系统提示词对实验结果有着不同的影响，但是通过FigStep仍然能有很好的ASR</li>
</ul>
<p><strong>对GPT-4V的越狱</strong></p>
<ul>
<li>提出FigStep Pro，将多个有害关键字分散嵌入不同的子图之中，来躲避GPT-4V的OCR检测</li>
</ul>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li><strong>有效性</strong>：FigStep可以在相同的VLM上获得平均82.50%的ASR，表明转移有害指令的模态确实可以绕过文本模块内的安全对齐</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>From neural activations to concepts: A survey on explaining concepts in neural networks</title>
    <url>/posts/1375690775/</url>
    <content><![CDATA[<p>​                      <img src="https://s2.loli.net/2025/10/23/2MXrpwNePI8ycYq.png" alt="image.png" style="zoom: 50%;" />          </p>
<blockquote>
<p>From neural activations to concepts: A survey on explaining concepts in neural</p>
</blockquote>
<span id="more"></span>
<h1 id="From-neural-activations-to-concepts-A-survey-on-explaining-concepts-in-neural-networks"><a href="#From-neural-activations-to-concepts-A-survey-on-explaining-concepts-in-neural-networks" class="headerlink" title="From neural activations to concepts: A survey on explaining concepts in neural networks"></a>From neural activations to concepts: A survey on explaining concepts in neural networks</h1><blockquote>
<p><a href="https://journals.sagepub.com/doi/pdf/10.3233/NAI-240743">https://journals.sagepub.com/doi/pdf/10.3233/NAI-240743</a></p>
</blockquote>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>人工智能的不可解释性</li>
<li>解释神经网络如何学习概念，因为概念可以作为构建复杂规则的原语</li>
<li>识别的概念可以帮助推理器干预神经网络，从而通过修改概念来调试网络</li>
</ul>
<h2 id="Neuron-level-explanations"><a href="#Neuron-level-explanations" class="headerlink" title="Neuron-level explanations"></a>Neuron-level explanations</h2><h3 id="Using-similarities-between-concepts-and-activations"><a href="#Using-similarities-between-concepts-and-activations" class="headerlink" title="Using similarities between concepts and activations"></a>Using similarities between concepts and activations</h3><ul>
<li>使用概念和激活之间的相似性进行神经元级解释，用输入文字和图像得到激活图，最后与原始图像的mask进行对比，如果 IoU 值高于给定阈值，则卷积滤波器表示概念 C。</li>
</ul>
<p><img src="https://s2.loli.net/2025/10/23/2MXrpwNePI8ycYq.png" alt=""></p>
<ul>
<li>怀疑会不会不只是一个卷积滤波器表示一个概念，而是一些卷积滤波器的线性组合</li>
<li>利用CLIP, 因为CLIP 将图像和文本嵌入到同一矢量空间中，从而可以测量文本和图像之间的相似性，解释卷积滤波器所代表的概念，他们为滤波器选择一组激活度最高的图像，然后使用CLIP测量的图像与每个概念进行匹配，最后找到最匹配的概念。</li>
</ul>
<h3 id="Using-causal-relationships-between-concepts-and-activations"><a href="#Using-causal-relationships-between-concepts-and-activations" class="headerlink" title="Using causal relationships between concepts and activations"></a>Using causal relationships between concepts and activations</h3><ul>
<li>分析通过干预输入和测量神经激活来展现输入概念和神经元之间的因果关系</li>
<li><p>方法1：将每个句子分解为一组连续的单词序列来提取概念，这些单词序列形成一个有意义的块。然后，通过首先重复概念以创建一个固定长度的合成句子（以规范不同概念的输入对单位的贡献），然后测量过滤器激活的平均值来衡量每个概念对过滤器激活的贡献。</p>
</li>
<li><p>方法2：首先将神经元的激活修改为在对输入进行干预时神经元将输出的激活（例如，输入句子中的主语从单数更改为复数），然后测量有和没有干预的动词正确变位预测之间的变化量</p>
</li>
<li>GPT-2 中中间层前馈模块中的神经元与编码事实信息和实现权重修饰符以改变权重值和改变事实知识最相关。</li>
</ul>
<p><img src="https://s2.loli.net/2025/10/23/IoPpS7VADHRxh1G.png" alt=""></p>
<h2 id="Layer-level-explanations"><a href="#Layer-level-explanations" class="headerlink" title="Layer-level explanations"></a>Layer-level explanations</h2><ul>
<li>概念也可以用整层来表示，而不是神经元或卷积滤波器。</li>
<li>在层级解释中，有两种方法很突出：第一种是用概念激活向量进行解释，第二种是探测。两种方法之间的主要区别在于，线性二进制分类器是针对每个概念训练的；而在探测中，多类分类器是使用通常与某些语言特征（例如，情感，词性标签）相关的分类标签进行训练的。</li>
</ul>
<h3 id="Using-vectors-to-explain-concepts-Concept-activation-vectors（概念激活向量）"><a href="#Using-vectors-to-explain-concepts-Concept-activation-vectors（概念激活向量）" class="headerlink" title="Using vectors to explain concepts: Concept activation vectors（概念激活向量）"></a>Using vectors to explain concepts: Concept activation vectors（概念激活向量）</h3><ul>
<li><p>概念激活向量（CAV），是一种用于<strong>神经网络层级概念解释</strong>的连续向量。其核心逻辑如下</p>
<p><strong>1. 网络结构分解：</strong><br>将神经网络 $f$ 拆分为两部分 $f = f^{\top} \circ f^{\perp}$，其中 $f^{\perp}: \mathbb{R}^m \rightarrow \mathbb{R}^n$ 是“底层网络”，包含我们感兴趣的卷积层 $l$；$f^{\top}$ 是“顶层输出层”。</p>
<p><strong>2. 概念识别流程：</strong>  </p>
<p>为了识别某一概念 $C$（如“条纹”“鸟类”），需准备正例 $x_C^{+}$（含概念 $C$）和负例 $x_C^{-}$（不含概念 $C$）。将其输入 $f^{\perp}$ 后，收集对应激活 $f^{\perp}(x_C^{+}) \in \mathbb{R}^n$ 和 $f^{\perp}(x_C^{-}) \in \mathbb{R}^n$。</p>
<p><strong>3. CAV 的生成：</strong>  </p>
<p>训练一个线性分类器来区分 $f^{\perp}(x_C^{+})$ 和 $f^{\perp}(x_C^{-})$，该分类器决策边界的法向量 $v_C \in \mathbb{R}^n$ 即为概念 $C$ 的概念激活向量（CAV）。</p>
</li>
<li><p>CAV 的核心应用之一是测试输入与概念的相关性（<strong>TCAV, Testing with CAVs</strong>），用于衡量“概念 $C$ 对类别标签 $k$ 预测的影响程度”。其原理是：对数据集 $\mathcal{X}$ 中所有类别为 $k$ 的图像 $x$，将其潜在向量 $f^{\perp}(x)$ 沿 $v_C$ 方向移动（即 $f^{\perp}(x) + \varepsilon \cdot v_C$），观察 $f^{\top}$ 对标签 $k$ 的对数概率变化，以此量化概念 $C$ 对类别 $k$​ 预测的“正影响概率”。</p>
<blockquote>
<p>比如说：</p>
<p>假设我们想知道“鸟类特征（$C$）对‘翠鸟类别（$k$）’的预测有多大正影响”，过程如下：</p>
<p><strong>步骤 1：</strong> 取所有真实类别为“翠鸟”的图像 $x$，即一批翠鸟的照片。  </p>
<p><strong>步骤 2：</strong> 对每个 $x$，计算其潜在向量 $f^{\perp}(x)$，得到这些翠鸟在“底层特征空间”中的表示。  </p>
<p><strong>步骤 3：</strong> 沿 $v_C$ 方向移动潜在向量，即计算 $f^{\perp}(x) + \varepsilon \cdot v_C$，相当于“为每张翠鸟图片添加更多‘鸟类共性特征’（如羽毛纹理、喙的形状等）”。  </p>
<p><strong>步骤 4：</strong> 将移动后的向量输入 $f^{\top}$，观察类别 $k$ 的对数概率。如果模型对“翠鸟（$k$）”的对数概率上升，说明“注入鸟类特征后，模型更确信这是翠鸟”，即概念 $C$ 对类别 $k$ 有正影响。  </p>
<p><strong>步骤 5：</strong> 统计“正影响”的占比：在所有真实为翠鸟的图像中，计算满足“注入 $C$ 后对数概率上升”的样本比例，该比例即为“概念 $C$ 对类别 $k$ 的正影响概率”。</p>
</blockquote>
</li>
<li><p>这一过程的本质是<strong>量化 “概念 C 的存在，多大程度上让模型更倾向于预测类别 k”</strong>。如果正影响概率很高，说明 “概念 C 是类别 k 的强特征”（比如 “鸟类特征” 对 “翠鸟” 分类的正影响概率很高）；反之，则说明概念 C 对类别 k 的预测贡献很小。</p>
</li>
<li>问题：原 CAV 方法需手动准备 “带概念标签的图像数据集”，改进<ul>
<li>无标签概念的自动生成<ul>
<li>对同一类别的图像进行<strong>多分辨率分割</strong>，将分割后的 “区域簇” 作为 “隐式概念” 用于 TCAV</li>
</ul>
</li>
<li>端到端的 CAV 与模型联合训练<ul>
<li>不再单独准备概念数据集，而是在 “原始图像分类任务” 中同时训练 CAV 和模型。具体来说，计算 “向量值分数”（每个值对应一个可学习的概念，反映卷积层感受野中该概念的存在程度），并将分数输入多层感知机（MLP）完成分类，从而实现 “概念学习与分类任务的协同优化”。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/10/23/YRwBJOsVT1jh4Ez.png" alt=""></p>
<h3 id="Using-classifiers-to-explain-concepts-Probing（探测）"><a href="#Using-classifiers-to-explain-concepts-Probing（探测）" class="headerlink" title="Using classifiers to explain concepts: Probing（探测）"></a>Using classifiers to explain concepts: Probing（探测）</h3><ul>
<li>探测使用分类器来解释概念。然而，探测不是为每个概念训练二进制线性分类器来测量概念在层激活中的存在，而是使用分类器进行多类分类</li>
<li>其标签通常代表 NLP 中的语言特征（例如，情感、词性标签）</li>
</ul>
<p><img src="https://s2.loli.net/2025/10/23/qt4JYK3SoGRdQwn.png" alt=""></p>
<h3 id="Using-localist-representations-Concept-bottleneck-models-概念瓶颈模型"><a href="#Using-localist-representations-Concept-bottleneck-models-概念瓶颈模型" class="headerlink" title="Using localist representations: Concept bottleneck models (概念瓶颈模型)"></a>Using localist representations: Concept bottleneck models (概念瓶颈模型)</h3><ul>
<li><p>每个概念都由模型f的瓶颈层中的一个唯一神经元表示</p>
</li>
<li><p>概念瓶颈模型（<strong>Concept Bottleneck Model, CBM</strong>）的整体结构可拆解为三个部分，通过函数组合（$\circ$）构成完整模型 $f = f^{\top} \circ f_{l} \circ f^{\perp}$。各部分功能如下：</p>
<p><strong>1. 底部模块（$f^{\perp}$）：</strong>  </p>
<p>通常由预训练模型的底层构成，作用是从输入（如图片、文本）中提取基础特征。</p>
<p><strong>2. 瓶颈层（$f_{l}$）：</strong>  </p>
<p>作为核心模块，一般是一个线性层。它将 $f^{\perp}$ 提取的基础特征映射到“概念空间”，输出每个概念在当前输入中的强度（例如：“这张图中‘猫’的概念强度是 0.8，‘狗’是 0.1”）。</p>
<p><strong>3. 顶部预测器（$f^{\top}$）：</strong>  </p>
<p>接收瓶颈层输出的“概念强度”，并基于这些概念进行最终任务预测（如分类或回归）。  </p>
<p>这种结构的优势在于<strong>可解释性</strong>——能够明确知道是哪些概念（如“有尾巴”“有胡须”）导致了模型的预测结果（如“这是猫”）。</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2025/10/23/96FX8svueKUYRpG.png" alt=""></p>
<ul>
<li><strong>核心局限</strong>：训练瓶颈层时，除了任务标签（如 “这是猫”），还需要<strong>概念标签（如 “图中有尾巴、有胡须”）</strong>。但很多任务中，人工标注概念标签成本高或难以获取。</li>
<li><strong>最新解决思路</strong>：无需人工标注概念标签，而是通过<strong>外部资源自动获取任务相关的概念集</strong>，具体步骤如下：<ul>
<li>获取概念集，从外部资源中筛选与任务相关的概念，例如：<ul>
<li>知识库；</li>
<li>常见词汇库（如 2 万个英文常用词）；</li>
<li>大语言模型（如 GPT-3）生成的概念。</li>
</ul>
</li>
<li><strong>概念嵌入（$v_C$）</strong>：用 CLIP（一种视觉 - 语言模型）将每个概念词（如 “尾巴”“胡须”）转化为向量（$v_C$​）。</li>
<li><strong>计算概念强度</strong>：通过计算 “输入特征（f⊥(x)）” 与 “概念向量（$v_C$）” 的余弦相似度，自动得到输入 x 中每个概念 C 的强度，替代人工标注的概念标签。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>大模型理解</tag>
        <tag>神经元</tag>
      </tags>
  </entry>
  <entry>
    <title>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</title>
    <url>/posts/1243695164/</url>
    <content><![CDATA[<p>​                                         <img src="https://s2.loli.net/2025/01/10/gzdH9wc8PLeIrk4.png" alt="image.png" style="zoom: 80%;" />          </p>
<blockquote>
<p>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</p>
</blockquote>
<span id="more"></span>
<h2 id="Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation"><a href="#Gradient-Alignment-Improves-Test-Time-Adaptation-for-Medical-Image-Segmentation" class="headerlink" title="Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation"></a>Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation</h2><ul>
<li><a href="https://arxiv.org/pdf/2408.07343">阅读地址</a></li>
<li><p><a href="https://github.com/Chen-Ziyang/GraTa">代码</a></p>
</li>
<li><p>梯度对齐提高了医学图像分割的测试时间适应性</p>
</li>
</ul>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><ul>
<li><p>针对解决的问题：不同中心的医学图像之间普遍存在的领域转移的问题，阻碍了预训练模型的有效部署</p>
</li>
<li><p>原来提出的一些方法，次优优化方向和固定步长(学习率不变)，效果不是太好</p>
<blockquote>
<p>[!NOTE]</p>
<p>次优优化方向：选择的不是最佳的方向，而是一个相对较好的方向，基于当前信息，可能并不是全局最优的方向</p>
</blockquote>
</li>
<li><p>提出了基于梯度对齐时间自适应方法，提高优化过程中的梯度方向和学习率优化，出发点从自监督目标导出的伪梯度，我们的方法将辅助梯度与伪梯度结合在一起，以促进梯度对齐。</p>
</li>
<li><p>关键点</p>
<ul>
<li>相当于可以挖掘不同梯度之间的相似性，并矫正梯度方向，利用与当前分割任务相关的经验梯度</li>
<li>根据伪梯度和辅助梯度之间的余弦相似性设计了一个动态学习率，从而能够对不同测试数据上的预训练模型进行自适应微调</li>
</ul>
</li>
</ul>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li><p>主要提高泛化能力</p>
<blockquote>
<p>[!NOTE]</p>
<p>TTA: 测试时间自适应，用在迁移学习和无监督学习中，核心思想是模型在处于推理阶段时，对模型进行自适应调整，不依赖于训练阶段的标签信息，而是根据测试阶段的输入做出动态调整 </p>
</blockquote>
</li>
<li><p>认为原来的方法的主要问题是，忽略了优化过程中的两个关键因素：步长和方向</p>
<p><img src="https://s2.loli.net/2025/01/10/gzdH9wc8PLeIrk4.png" alt=""></p>
</li>
<li><p>主要方法对比，如上图：</p>
<ul>
<li>a图蓝色的线代表需要优化的伪梯度，而粉色的线代表经验梯度的方式，原来采取的方式是直接优化伪梯度(弱增强和强增强两种，一致性损失产生的梯度)的方式</li>
<li>b图则是借助一个辅助梯度(熵损失,原有测试集中导出的梯度)，最小化辅助梯度(黄色的线)和伪优化的梯度(蓝色的线)之间的角度，而因为这两个会把粉红色的线(经验梯度)包含在其中，从而达到蓝色的线(原有方式)和粉色的线(经验梯度)两者不断靠近</li>
</ul>
</li>
<li><p>还提出了一个与这两个梯度之间的角度成反比的动态学习率，以自适应地微调预训练模型。角度更大意味着冲突更大，则需要更小的学习率慢慢地来调</p>
</li>
<li><p>主要贡献;</p>
<ul>
<li>提出GraTa来改进优化方向和步长</li>
<li>有不同的伪梯度和辅助梯度两种，减少任务的分歧</li>
<li>不同梯度之间角度，对两个梯度之间的余弦相似性的可变学习率，有助于动态确定自适应微调的优化步长</li>
</ul>
</li>
</ul>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><ul>
<li>原有的TTA方法主要是<ul>
<li>基于构建自我监督的辅助任务来微调模型</li>
<li>微调批归一化层内的仿射参数</li>
<li>这样的方法但是存在一定的问题，当监督信息可靠的时候，这些方法表现出有效适应性，不可靠的时候，性能便会下降</li>
</ul>
</li>
<li>原有梯度对齐的方法<ul>
<li>对梯度进行投影，缓解任务梯度之间的梯度冲突的问题</li>
<li>联邦学习加上隐式正则化的方式</li>
</ul>
</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="损失函数的定义"><a href="#损失函数的定义" class="headerlink" title="损失函数的定义"></a>损失函数的定义</h4><ul>
<li><p><script type="math/tex">\text{Lent}(\theta; X_i^t) = -P_i \log P_i, \quad P_i = \text{Sigmoid}(f_\theta(X_i^t)).</script>​</p>
</li>
<li><p>测试样本的交叉熵损失函数</p>
</li>
<li><script type="math/tex; mode=display">\text{Lcon}(\theta; X_i^t) = \text{lce}(\tilde{P}_i, \hat{P}_i), \quad \tilde{P}_i = \text{Sigmoid}(f_\theta(\tilde{X}_i^t)), \quad \hat{P}_i = \frac{1}{j} \sum_{j=1}^j \text{Sigmoid}(f_\theta(\hat{X}_{ij}^t)).</script></li>
<li><p>弱增强和强增强两种，一致性损失产生的梯度</p>
</li>
<li><p>弱增强策略：身份映射，水平翻转，垂直翻转，旋转90度，旋转180度，旋转270度</p>
</li>
<li><p>强增强策略：亮度调整，对比度调整，伽马变化，高斯噪声和高斯模糊</p>
</li>
<li><script type="math/tex; mode=display">\min_{\theta} \left( \text{Lcon}(\theta; X_i^t) + \angle(\nabla \text{Lcon}(\theta; X_i^t), \nabla \text{Lent}(\theta; X_i^t)) \right)</script></li>
<li><p>最后的目标参数</p>
</li>
<li><p>但是优化的时候角度不能直接微分，而且如果采用内积的方式的话，计算复杂而且不稳定，因此采取下面的这种方式</p>
<p><script type="math/tex">\min_{\theta} \, \text{Lcon}(\theta'; X_i^t), \quad \text{where} \quad \theta' = \theta - \nabla \text{Lent}(\theta; X_i^t).</script>​</p>
</li>
<li><p>但是最后在对比试验加上消融实验之后得出下面的变体的效果是最好的<img src="C:\Users\yby\AppData\Roaming\Typora\typora-user-images\image-20250111122115949.png" alt="image-20250111122115949"></p>
</li>
</ul>
<h4 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h4><ul>
<li><p>微调过程</p>
<ul>
<li><p>自己的理解是，就相当于上面的那个式子两个部分先后进行计算，然后再反向传播</p>
</li>
<li><p>首先计算<script type="math/tex">\text{Lent}(\theta; X_i^t)</script>，然后反向传播，更新参数</p>
</li>
<li><p>然后再计算<script type="math/tex">\text{Lcon}(\theta'; X_i^t)</script>,  然后再反向传播，得到学习率，然后再更新参数 <script type="math/tex">\theta_{i+1} \leftarrow \theta_i - \eta \nabla \text{Lcon}(\theta_i'; X_i^t)</script></p>
</li>
<li><p>然后又进行推理 </p>
</li>
</ul>
</li>
<li><p>参数<script type="math/tex">\eta</script></p>
</li>
<li><script type="math/tex; mode=display">\eta = \beta \cdot \text{Cus} \left( \frac{\nabla \text{Lcon}(\theta'; X_i^t) \cdot \nabla \text{Lent}(\theta; X_i^t)}{\|\nabla \text{Lcon}(\theta'; X_i^t)\| \, \|\nabla \text{Lent}(\theta; X_i^t)\|} \right)</script></li>
<li><p>β是一个缩放因子</p>
</li>
</ul>
<p>过程总览图</p>
<p><img src="C:\Users\yby\AppData\Roaming\Typora\typora-user-images\image-20250110211655820.png" alt="image-20250110211655820"></p>
<h3 id="测评"><a href="#测评" class="headerlink" title="测评"></a>测评</h3><ul>
<li><p>对于每幅图像，裁剪了一个以OD为中心的感兴趣区域（ROI），大小为800×800，然后将每个ROI进一步调整为512×512，并通过最小最大归一化进行归一化</p>
</li>
<li><p>用DSC公式进行估计</p>
<blockquote>
<p>[!NOTE]</p>
<p>DSC (<strong>Dice Similarity Coefficient</strong>)，一种用于衡量两个样本的相似度的指标，尤其常用于图像分割任务中。它是一个衡量预测结果与真实标签之间重叠程度的标准。</p>
<p><script type="math/tex">DSC = \frac{2 \times |A \cap B|}{|A| + |B|}</script>​</p>
<p>A 是预测的分割区域，B 是真实的分割区域</p>
</blockquote>
</li>
<li><p>ResUNet-34作为基线模型</p>
</li>
<li><p>与BN-based, Fine-tune-based 对比，效果基本上都是最好的</p>
</li>
<li><p>消融实现，测试样本的交叉熵损失函数选什么，最后得出改变权重更换的方向其实涨点不是太明显，而是动态学习率涨点很多</p>
</li>
</ul>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul>
<li>可以想想其他的多任务的优化上面，可不可以采取这样的方式</li>
<li>弱增强和强增强的手段不同，会不会对涨点有影响</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Test-Time Adaptation（TTA）</tag>
      </tags>
  </entry>
  <entry>
    <title>MM-SafetyBench的论文阅读</title>
    <url>/posts/2910022633/</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/11/23/LSlDt9MCFVzocag.png" alt=""></p>
<blockquote>
<p>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models 论文学习</p>
</blockquote>
<span id="more"></span>
<h2 id="MM-SafetyBench-A-Benchmark-for-Safety-Evaluation-of-Multimodal-Large-Language-Models"><a href="#MM-SafetyBench-A-Benchmark-for-Safety-Evaluation-of-Multimodal-Large-Language-Models" class="headerlink" title="MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"></a>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</h2><p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<p><a href="https://arxiv.org/pdf/2311.17600">原文链接</a></p>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<h4 id="1-1视觉模块和语言模块连接方式"><a href="#1-1视觉模块和语言模块连接方式" class="headerlink" title="1.1视觉模块和语言模块连接方式"></a>1.1视觉模块和语言模块连接方式</h4><ol>
<li><strong>线性投影</strong>：使用线性投影将视觉标记 (visual tokens) 的维度与文本标记 (text tokens) 的维度对齐</li>
<li><strong>可学习的查询</strong>：使用可学习的查询来提取与文本相关的视觉信息，并固定视觉标记的长度。</li>
<li><strong>利用Few-shot</strong>: 利用few-shot使模形快速适应新的任务</li>
</ol>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文出于对多模态大模型很容易被破坏的缘故，提出了MM SafetyBench数据集，用于对MLLM进行安全评估，总共有13个场景，5040个文本图像对。还提出用用扩散模型和排版生成的图像，来创建图像提示，以绕过MLLM中的安全性防御机制</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<ul>
<li><strong>差距</strong>：LLM的安全问题得到了广泛讨论，还有安全对齐的措施，但是MLLM的安全问题仍然研究不足</li>
<li><strong>评估</strong>：缺乏一个较好的数据集对MLLM进行安全性评估</li>
<li><strong>动机</strong>：当图片与文字相关时，模型会激活视觉模块，这一模块，通常没有进行安全性对齐；当两者不相关的时候，语言模块占据主导地位，导致攻击失败</li>
</ul>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<h4 id="4-1-有效性"><a href="#4-1-有效性" class="headerlink" title="4.1 有效性"></a>4.1 有效性</h4><p>当使用图像文本对的时候，对MLLM的攻击显得就很有效</p>
<p><img src="https://s2.loli.net/2024/11/23/LSlDt9MCFVzocag.png" alt=""></p>
<h4 id="4-2-数据集的构建"><a href="#4-2-数据集的构建" class="headerlink" title="4.2 数据集的构建"></a>4.2 数据集的构建</h4><ol>
<li><strong>生成问题</strong>：使用 GPT-4 来生成问题，而且一个问题对应的是三个图像</li>
<li><strong>提取关键短语</strong>：<ul>
<li>首先，有两种不同的场景<ul>
<li>每个问题都包含一个有害短语</li>
<li>每个问题都包含一个政治话题</li>
</ul>
</li>
<li>然后提取之后用于第三步，图像的生成</li>
</ul>
</li>
<li><strong>查询到图像的转换</strong>：<ul>
<li>基于扩散模型</li>
<li>排版图形，使用Pillow，来对图像进行绘图</li>
<li>扩散模型+排版图形，将两者连接在一起，扩散模型在上面，排版图形在下面</li>
</ul>
</li>
<li><strong>问题的改写</strong>：根据第一步的问题和第三步生成的图片进行改写生成新的问题</li>
</ol>
<h4 id="4-3-模型的评估"><a href="#4-3-模型的评估" class="headerlink" title="4.3 模型的评估"></a>4.3 模型的评估</h4><ol>
<li>将场景分为三个类别，不同的类别，认为安全的方式不同<ul>
<li>对于一些类别，如非法活动，不包含任何有害内容，则认为是安全的</li>
<li>还有有些类别，如政治话题，不响应则认为是安全的</li>
<li>最后的一些类别，如法律医疗领域，包含免责声明和风险警告则是安全的</li>
</ul>
</li>
<li>利用ASR，平均攻击成功率来评估模型</li>
<li>利用RR，拒绝率，来反映模型是否准确的识别到恶意查询，并做出拒绝</li>
</ol>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>模型设置</strong>：评估了最近发布的12种模型</p>
<p><strong>实验方式</strong>：使用排版，扩散，基线等方式</p>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li><strong>safety prompt</strong>：提出了safety prompt，使模型能够更好的抵御攻击，这一结果是建立在模型能够遵循指令的条件下</li>
<li><strong>扩散+排版</strong>大多数情况下效果最好，相比于基线还有单纯的用基线和排版</li>
<li>没有一个模型能够做到<strong>安全性和智能性</strong>的平衡，详细说来，则是有些模型看似安全实则无法正确做出相应</li>
<li>附录中还对MLLM Protector做了对比，发现 Safety prompt 即激发自主的安全防御更有效</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</strong></p>
<ul>
<li>介绍FigStep算法，通过将有害指令转换为图像来对VLM进行攻击，并且证明出在具有OCR功能的GPT-4V上也有较高攻击率。</li>
</ul>
<p><strong>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</strong></p>
<ul>
<li>提出MM-SafetyBench数据集，用来评估多模态大模型，发现其容易受到图文结合的攻击，特别是与文字相关的图片，最后还提出了可以引入安全提示减少攻击成功率。</li>
</ul>
<p><strong>Visual Adversarial Examples Jailbreak Aligned Large Language Models</strong></p>
<ul>
<li>主要是从对抗性样本上面出发，发现其可以绕开VLM的安全防御，使模形能够生成与训练无关的有害内容，还解锁了模型被禁止的能力。</li>
</ul>
<p><strong>对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Visual Adversarial Examples (2023.8)</th>
<th style="text-align:center">FigStep (2023.12)</th>
<th style="text-align:center">MM-SafetyBench (2024.6)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>研究对象</strong></td>
<td style="text-align:center">VLM</td>
<td style="text-align:center">VLM</td>
<td style="text-align:center">MLLM(但主要还是针对VLM)</td>
</tr>
<tr>
<td style="text-align:center"><strong>方法特点</strong></td>
<td style="text-align:center">对抗性图片</td>
<td style="text-align:center">图形化有害文字</td>
<td style="text-align:center">文本+图片 (扩散+排版)</td>
</tr>
<tr>
<td style="text-align:center"><strong>创新点</strong></td>
<td style="text-align:center">强调对抗性样本的通用性</td>
<td style="text-align:center">简单的图形化文字的攻击</td>
<td style="text-align:center">提出安全评估基准</td>
</tr>
<tr>
<td style="text-align:center"><strong>研究模型范围</strong></td>
<td style="text-align:center">GPT-4V和LLaVA</td>
<td style="text-align:center">6种VLM+GPT-4V</td>
<td style="text-align:center">12种开源MLLM</td>
</tr>
<tr>
<td style="text-align:center"><strong>评估方法</strong></td>
<td style="text-align:center">人工检查与黑盒传递性验证</td>
<td style="text-align:center">SafeBench基准测试</td>
<td style="text-align:center">MM SafetyBench数据集(13个场景，5040个文本图像对)</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>TEST-TIME ADAPTATION FOR IMAGE COMPRESSION WITH DISTRIBUTION REGULARIZATION</title>
    <url>/posts/3200710163/</url>
    <content><![CDATA[<p>​    <img src="https://s2.loli.net/2025/10/23/JqwKfdPGbIn3Re7.png" alt="image.png" style="zoom: 60%;" />          </p>
<blockquote>
<p>TEST-TIME ADAPTATION FOR IMAGE COMPRESSION WITH DISTRIBUTION REGULARIZATION</p>
</blockquote>
<span id="more"></span>
<h1 id="TEST-TIME-ADAPTATION-FOR-IMAGE-COMPRESSION-WITH-DISTRIBUTION-REGULARIZATION"><a href="#TEST-TIME-ADAPTATION-FOR-IMAGE-COMPRESSION-WITH-DISTRIBUTION-REGULARIZATION" class="headerlink" title="TEST-TIME ADAPTATION FOR IMAGE COMPRESSION WITH DISTRIBUTION REGULARIZATION"></a>TEST-TIME ADAPTATION FOR IMAGE COMPRESSION WITH DISTRIBUTION REGULARIZATION</h1><h2 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1. 背景知识"></a>1. 背景知识</h2><ul>
<li><p>现有基于先验的熵模型在跨域场景中因分布偏移导致联合概率分布不准确，进而影响性能。</p>
<ul>
<li><strong>先验（Hyperprior）</strong>：先验是一种用于图像压缩的模型，它通过引入一个额外的先验概率模型来捕获潜在表示中的空间依赖性。这个先验模型通常用于编码潜在表示的尺度信息，从而提高压缩效率。</li>
<li><strong>熵模型</strong>：熵模型用于估计潜在表示的分布，以便进行有效的熵编码。在基于超先验的模型中，熵模型通常假设潜在表示的元素是独立的，但实际上这些元素之间可能存在统计依赖性。</li>
</ul>
</li>
<li><p>传统联合优化潜在变量和辅助信息的方法（如HLR）在域内有效，但在跨域任务中因概率分布失配导致码率显著增加。</p>
<ul>
<li><p>在跨域任务中，由于训练数据和测试数据的分布不同，潜在变量和辅助信息的概率分布也会发生变化。例如，自然图像和屏幕内容图像在统计特性上存在显著差异。当模型在自然图像上训练后，其潜在变量和辅助信息的概率分布是基于自然图像的特性学习的。然而，当模型应用于屏幕内容图像时，这些概率分布不再适用，导致模型无法准确地编码潜在变量和辅助信息。</p>
<p><img src="https://s2.loli.net/2025/10/23/JqwKfdPGbIn3Re7.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h2 id="2-研究方法"><a href="#2-研究方法" class="headerlink" title="2. 研究方法"></a>2. 研究方法</h2><h3 id="边缘化近似角度揭示降解原因"><a href="#边缘化近似角度揭示降解原因" class="headerlink" title="边缘化近似角度揭示降解原因"></a>边缘化近似角度揭示降解原因</h3><ul>
<li><strong>边缘化近似</strong>：在图像压缩中，边缘化近似是指通过将联合概率分布 <em>p</em>(<em>y</em>,<em>z</em>) 近似为边缘概率分布 <em>p</em>(<em>y</em>) 和 <em>p</em>(<em>z</em>) 的乘积，来简化模型的计算。其中，<em>y</em> 表示潜在变量，<em>z</em> 表示超先验中的侧信息。</li>
<li><strong>降解原因</strong>：在跨域场景中，由于源域和目标域之间的分布差异，基于超先验的熵模型无法准确地捕获潜在变量和侧信息的联合概率分布。这导致了边缘化近似的不准确，进而影响了码率（Rate）和重构质量（Distortion）的权衡（R-D性能）。具体来说，模型在编码潜在变量和侧信息时需要更多的比特来表示这些变量，从而导致码率显著增加</li>
</ul>
<h4 id="分布正则化方法"><a href="#分布正则化方法" class="headerlink" title="分布正则化方法"></a>分布正则化方法</h4><ul>
<li><p>分布正则化通过对模型参数引入先验分布，分布正则化通过鼓励学习更好的联合概率近似，来减少跨域场景中的额外率消耗。</p>
</li>
<li><p>例如，通过分布正则化，可以调整潜在变量的分布，使其更接近目标域的分布，从而在跨域压缩中实现更好的率失真性能。</p>
<ul>
<li>zmt：表示在第 <em>m</em> 次迭代中更新的<strong>超先验</strong>（hyperprior）中的<strong>侧信息</strong>（side information）。侧信息 <em>z</em> 是从潜在变量 <em>y</em> 中提取的，用于帮助编码和解码过程。在跨域任务中，由于分布偏移，<em>z</em> 的概率分布可能与训练时的分布不同，导致编码效率下降。</li>
<li>ymt：表示在第 <em>m</em> 次迭代中更新的<strong>潜在变量</strong>。潜在变量 <em>y</em> 是图像经过编码器变换后的表示，用于捕获图像的重要特征。在跨域任务中，由于分布偏移，<em>y</em> 的概率分布也可能与训练时的分布不同，导致编码效率下降。</li>
</ul>
</li>
<li><h3 id="β-的作用"><a href="#β-的作用" class="headerlink" title="β 的作用"></a>β 的作用</h3><ul>
<li><strong>控制正则化强度</strong>：<em>β</em> 用于控制正则化项 −log<em>p</em>(<em>z<strong>m</strong>t</em>∣<em>y<strong>m</strong>t</em>) 在总损失函数中的权重。较大的 <em>β</em> 值会增加正则化项的影响，从而更严格地限制模型的复杂度，有助于防止过拟合。</li>
<li><strong>平衡模型复杂度和泛化能力</strong>：通过调整 <em>β</em>，可以平衡模型的复杂度和泛化能力。较小的 <em>β</em> 值可能会导致模型过拟合，而较大的 <em>β</em> 值可能会导致模型欠拟合。</li>
<li>最好0.1</li>
</ul>
</li>
</ul>
<h4 id="Dropout变分推断"><a href="#Dropout变分推断" class="headerlink" title="- Dropout变分推断"></a>- Dropout变分推断</h4><ul>
<li>文章利用dropout变分推断（DVI）作为贝叶斯近似，来估计潜在变量的后验分布。通过多次蒙特卡洛（MC）采样，估计后验分布的均值和方差，从而实现对潜在变量的优化。</li>
<li>例如，DVI通过随机丢弃神经元的方式，模拟潜在变量的不确定性，为潜在变量的优化提供了更准确的估计。</li>
</ul>
<h2 id="3-实验与结果"><a href="#3-实验与结果" class="headerlink" title="3. 实验与结果"></a>3. 实验与结果</h2><h3 id="3-1-数据集"><a href="#3-1-数据集" class="headerlink" title="3.1 数据集"></a>3.1 数据集</h3><ul>
<li>文章使用了六个不同的数据集，包括自然图像（Kodak）、屏幕内容图像（SIQAD、SCID、CCT）、像素风格游戏图像和绘画图像（DomainNet）</li>
</ul>
<h3 id="3-3-性能评估"><a href="#3-3-性能评估" class="headerlink" title="3.3 性能评估"></a>3.3 性能评估</h3><h4 id="3-3-1-R-D曲线与BD-rate"><a href="#3-3-1-R-D曲线与BD-rate" class="headerlink" title="3.3.1 R-D曲线与BD-rate"></a>3.3.1 R-D曲线与BD-rate</h4><ul>
<li>通过计算不同方法的峰值信噪比（PSNR）和每像素比特数（bpp），绘制R-D曲线，并计算（BD-rate）来评估性能。BD-rate越低，表示性能越好。</li>
</ul>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ul>
<li><p>超参数β和MC采样次数T对性能影响显著（如表3），但文中未提供自动调参方案。此外，不同数据集可能需特定参数配置，限制了方法的即插即用性。</p>
</li>
<li><p>dropout变分推断是什么意思，我的理解是测试的时候也用dropout, 只是换了种解释方法</p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>Test-Time Adaptation（TTA）</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Adversarial Examples的论文阅读</title>
    <url>/posts/542407868/</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/11/25/wrEodCjUzDWuhBl.png" alt=""></p>
<blockquote>
<p>Visual Adversarial Examples Jailbreak Aligned Large Language Models 论文学习</p>
</blockquote>
<span id="more"></span>
<h2 id="Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models"><a href="#Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models" class="headerlink" title="Visual Adversarial Examples Jailbreak Aligned Large Language Models"></a>Visual Adversarial Examples Jailbreak Aligned Large Language Models</h2><blockquote>
<p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<p><a href="https://arxiv.org/pdf/2306.13213">文章链接</a></p>
<p><a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models">论文代码</a></p>
</blockquote>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<p><strong>对抗性样本</strong>：</p>
<ul>
<li>对抗性样本是指在原始数据上添加特定的干扰，这个干扰是像素级别的修改，人几乎看不出来变化，但是会对模型产生误导</li>
<li>一般是基于梯度来生成，使之损失变大</li>
</ul>
<p><strong>红队攻击（Red-teaming）</strong>：</p>
<p>在AI和机器学习中，红队攻击指的是通过模拟恶意用户的行为，主动寻找并利用模型的潜在弱点</p>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文在多模态大模型的产生下，提出了对抗性攻击，这种攻击通过改变图片中的像素，来进行攻击，这种攻击被证明是非常有效的。与此同时，这种对抗性攻击，还有通用性，对绝大多数的对齐模型都能迁移起作用。最后还展望了对其他种类模态信息的探究。</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<p><strong>革命</strong>：最近人们将视觉融合到LLM中的兴趣激增，导致VLM的出现。</p>
<p><strong>复杂性</strong>：然而，多模态模型增加了模型的复杂度和攻击面，这对于确保模型的安全性和可靠性构成了挑战。</p>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<p><strong>视觉输入</strong>：因为视觉输入的连续性和高维性，使之成为对抗攻击的薄弱之处</p>
<p><strong>白盒攻击</strong>：知道模型权重，便于进行生成对抗性样本</p>
<p><strong>黑盒攻击</strong>：不知道模型权重，但是这个攻击可以在多个模型之间移植，实现全面性，实现更加通用的攻击</p>
<p><strong>对抗性样本</strong>：针对的并不是一种特殊的指令，而是一种通用的指令，能够使绝大多数有害文本可以得到输出</p>
<p><strong>对抗性样本构建方式</strong>：</p>
<ul>
<li><p>攻击的目的是构建一个对抗性输入 <script type="math/tex">x_{\text{adv}}</script>，使模型在给定 <script type="math/tex">x_{\text{adv}}</script>时，倾向于生成一些有害内容 Y=<script type="math/tex">\{y_i\}_{i=1}^m</script>​</p>
</li>
<li><p>Y是一个小型语料库，其中包含一些有害的语句，这些语句会用来指导攻击</p>
</li>
<li><p>通过调整<script type="math/tex">x_{\text{adv}}</script>来最大化模型生成有害内容的概率</p>
</li>
<li><p>对于每个有害语句 <script type="math/tex">y_i</script>，它的生成概率用 <script type="math/tex">p(y_i \mid x_{\text{adv}})</script> 表示。通过取负对数<script type="math/tex">(−log)</script>​，问题变成了最小化目标函数</p>
</li>
<li><p>而且优化的过程受限于一个空间<strong><em>B</em></strong>，这个空间是用来让图片尽可能看起来正常</p>
<p><img src="https://s2.loli.net/2024/11/24/YBXuqJTO9rfK8jF.png" style="zoom: 50%;" /></p>
</li>
<li><p>最后再将 <script type="math/tex">x_{\text{adv}}</script>和 <script type="math/tex">x_{\text{harm}}</script>​一起送入模型</p>
</li>
</ul>
<p><strong>文本对抗攻击</strong></p>
<ul>
<li>将视觉对抗性嵌入替换为具有相等长度的文本对抗性标记</li>
<li>文本攻击的开销更大，因为文本在其空间更加离散，所以导致计算需求更高</li>
</ul>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>评估</strong></p>
<ul>
<li><p>三种形式，无对抗性信息，对抗性图像，对抗性文本，且从图中可以看出来，对图像施以越强的对抗，效果越好</p>
<p><img src="https://s2.loli.net/2024/11/23/fgjypQZhNrX4AMu.png" alt=""></p>
</li>
<li><p>用分类器计算毒性属性分数，范围从0到1，对每个对于每个属性，我们计算分数超过0.5阈值的生成文本的比率，并且重复三次</p>
</li>
</ul>
<p><strong>分析攻击</strong></p>
<ul>
<li>原来提出的一些削弱攻击的方法不再行之有效，例如对抗性训练 (将对抗性样本作为训练数据) ，因为现在的输出是开放的，与狭义的分类形成对比。而且现在的对抗性干扰不一定是不可察觉的，因此，这些防御所假设的小扰动界限不再适用。</li>
<li>但是通过向图像之中引入噪声能够在一定程度上中和对抗性攻击</li>
</ul>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li>传统的对纯文本输入输出的LLM的越狱攻击的难度比较大，因为文本是离散型的。在引入视觉机制后，对系统的越狱攻击就变得比较简单。</li>
<li>对抗性攻击，通过对图像的改变，来进行攻击是有效的</li>
</ul>
<h3 id="7-未来思考"><a href="#7-未来思考" class="headerlink" title="7. 未来思考"></a>7. 未来思考</h3><ul>
<li>多模态的出现，推测其他模态的信息比如语音等，也存在这样的跨模态攻击</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>Neuron-level Interpretation of Deep NLP Models: A Survey</title>
    <url>/posts/1183083500/</url>
    <content><![CDATA[<p>​      <img src="https://s2.loli.net/2025/10/22/8y5KbY1NxgahJLt.png" alt="image.png" style="zoom: 50%;" /></p>
<blockquote>
<p>Neuron-level Interpretation of Deep NLP Models: A Survey</p>
</blockquote>
<span id="more"></span>
<h1 id="Neuron-level-Interpretation-of-Deep-NLP-Models-A-Survey"><a href="#Neuron-level-Interpretation-of-Deep-NLP-Models-A-Survey" class="headerlink" title="Neuron-level Interpretation of Deep NLP Models: A Survey"></a>Neuron-level Interpretation of Deep NLP Models: A Survey</h1><blockquote>
<p><a href="https://arxiv.org/pdf/2108.13138">https://arxiv.org/pdf/2108.13138</a></p>
</blockquote>
<h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><ul>
<li>motivation<ul>
<li>随着深度神经网络在各个领域的激增，对这些模型的可解释性的需求也在增加</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li>在网络的神经元中学习了哪些概念？</li>
<li>是否有专门学习特定概念的神经元？</li>
<li>网络神经元内的知识是如何局部/分布和冗余地保存的？</li>
</ul>
</li>
<li><p>好处</p>
<ul>
<li><p>通过识别与预测相关的神经元来控制偏差和操纵系统行为</p>
</li>
<li><p>通过删除不太有用的神经元来进行模型蒸馏</p>
</li>
<li><p>通过选择神经元来进行有效的特征选择</p>
</li>
<li>通过用重要神经元引导搜索来进行神经结构搜索。</li>
</ul>
</li>
</ul>
<h2 id="基础定义"><a href="#基础定义" class="headerlink" title="基础定义"></a>基础定义</h2><ul>
<li><p><strong>聚焦神经元</strong>：学习单个概念的单个神经元</p>
</li>
<li><p><strong>群神经元</strong>：将组合起来表示一个概念的一组神经元</p>
<p><img src="https://s2.loli.net/2025/10/22/8y5KbY1NxgahJLt.png" alt=""></p>
</li>
</ul>
<h2 id="发现和理解网络中神经元的方法"><a href="#发现和理解网络中神经元的方法" class="headerlink" title="发现和理解网络中神经元的方法"></a>发现和理解网络中神经元的方法</h2><ul>
<li><p>方法</p>
<ul>
<li>可视化 - Visualization</li>
<li>基于语料库</li>
<li>基于探究</li>
<li>基于因果关系</li>
<li>杂项</li>
</ul>
</li>
<li><p>Visualization 可视化</p>
<ul>
<li>定性以及主观。</li>
<li>由于大量的人为参与，它无法扩展到整个网络。</li>
<li>很难解释在不同环境中获得多个角色的多义神经元。</li>
<li>它在识别组神经元方面无效。</li>
<li>并非所有神经元都是视觉上可解释的。</li>
</ul>
</li>
<li><p>Corpus-based Methods 基于语料库</p>
<ul>
<li>汇总数据激活的统计数据来发现神经元的作用</li>
<li>基于语料库的方法是全局解释方法，因为它们解释了神经元在一组输入中的作用</li>
<li>将神经元作为输入并识别神经元已经学习的概念的方法（概念搜索）<ul>
<li>简单来说，早期是 “看神经元对哪些 5 个词的组合最敏感，然后人来猜它代表什么”；后面则是 “看神经元对哪些完整句子最敏感，然后通过分析句子结构自动提炼出它代表的词或短语概念”，后者更高效，减少了人工依赖。</li>
</ul>
</li>
<li>将概念作为输入并标识学习概念的神经元的其他方法（神经元搜索）<ul>
<li>对语料中每个句子，根据神经元激活值阈值生成<strong>二值掩码（binary mask）</strong>。同样为每个概念生成二值掩码（标记其在句子中是否出现）。计算神经元掩码与概念掩码的<strong>交并比（IoU）</strong>，用于生成组合解释。</li>
<li>将神经元的<strong>原始激活值直接作为预测分数</strong>。计算每个神经元和每个概念的<strong>平均精度（Average Precision）</strong>。</li>
<li>聚焦于<strong>具有目标概念的实例</strong>（不是对语料库中的所有句子进行分析，而是先做一次 “筛选”—— 把要研究的 “概念”的句子挑出来。）计算神经元在这些实例上的<strong>平均激活值（Mean Activation Value）</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>Probing-based Methods 基于探究</p>
<ul>
<li><strong>Linear Classifiers</strong>：利用待分析模型生成的激活向量（可理解为模型中神经元的激活值组成的向量）来训练一个线性分类器，让它学习我们感兴趣的概念。分类器赋予神经元（作为分类器的特征）的权重，就代表了这些神经元对该概念的重要性分数。</li>
<li><strong>Gaussian Classifier</strong> ：其核心假设是：神经元的激活值服从高斯分布（正态分布）。他们的具体做法是，在所有神经元的激活值上拟合一个 “多元高斯分布”（即同时考虑多个神经元之间的关联，建模它们的联合分布），并为单个神经元提取了专门的 “探测指标”（用于衡量该神经元与目标概念的关联）。缺点：<strong>需要有监督数据来训练分类器</strong>。</li>
</ul>
</li>
<li><p>Causation-based methods 基于因果关系的方法 </p>
<ul>
<li><p>Ablation ：消融的核心逻辑是<strong>通过改变单个神经元的值，来观察它对模型整体性能的影响</strong>，以此判断该神经元的重要性。具体操作是将目标神经元的值 “钳位”（固定）为 0 或某个特定值，然后对比模型在操作前后的性能变化。</p>
<p>它主要有两种应用方向：</p>
<ul>
<li><strong>无监督场景：识别模型层面的关键神经元</strong>目标是找到对整个模型性能至关重要的神经元。判断标准是：当某个神经元被消融（值被固定）后，模型的整体性能（如准确率）出现大幅下降，这类神经元就是 “关键神经元”。</li>
<li><strong>有监督场景：识别特定输出类别的关键神经元</strong>目标是找到与模型某个具体输出类别（比如识别图片中的 “猫”）相关的神经元。判断标准是：当消融某个神经元后，模型对该类别的预测结果发生 “翻转”（比如原本预测为 “猫”，现在预测为 “狗”），这类神经元就是该类别对应的 “关键神经元”。此时，这个输出类别就是判断神经元重要性的 “参照概念”。</li>
</ul>
<p>消融技术的主要局限在于<strong>难以识别 “神经元组” 的作用</strong>。</p>
</li>
</ul>
</li>
<li><p><strong>Knowledge Attribution Method</strong>：</p>
<ul>
<li>模型（特指 Transformer 模型）所学习的事实性知识，是存储在其前馈神经网络（feed-forward neural networks）的神经元中的</li>
<li>使用了 “集成梯度（Integrated Gradient）” 这一归因技术，来计算并筛选出对表达某个 “关系事实” 最重要的那部分神经元（即 “top neurons”）。</li>
<li>只能识别出对预测重要的神经元，但无法直接知道这些神经元具体学习并代表了什么 “概念”。</li>
</ul>
</li>
<li><p><strong>Matrix Factorization</strong></p>
<ul>
<li>矩阵分解（MF）的核心是将一个大型矩阵分解为多个小型 “因子矩阵” 的乘积，其中每个 “因子” 代表一组功能相似的元素。</li>
<li>当输入一个句子时，模型各层的神经元激活值会构成一个<strong>激活矩阵</strong>。</li>
<li>MF 可用于分解这个激活矩阵，分解后得到的每个 “因子” 对应一组神经元，这组神经元共同学习并编码了某个特定的 “概念”（比如语法结构、语义角色等）。</li>
<li>而且目前几乎没有学术研究将 MF 方法用于分析 NLP 模型</li>
<li><strong>优势</strong>：与此前讨论的其他无监督解释方法相比，MF 的固有优势是能直接 “发现神经元组”（即通过分解自动将功能相似的神经元聚类），无需提前定义聚类规则。</li>
<li><strong>核心局限 </strong>：<ol>
<li>如何确定 “因子数量”（即需要将激活矩阵分解为多少个因子 / 神经元组）是一个非 trivial的问题，目前缺乏统一且有效的确定方法。</li>
<li>“局部解释” 意味着 MF 只能解释<strong>单个输入（如一个句子）对应的模型激活模式</strong>，而无法提供 “全局解释”（即跨多个输入的、模型普遍遵循的规律）</li>
</ol>
</li>
</ul>
</li>
<li><p>Clustering Methods</p>
<ul>
<li>聚类是另一种以无监督方式分析神经元组的有效方法。直觉是，如果一组神经元学习了一个特定的概念，那么它们的激活就会形成一个集群</li>
<li>局限性与矩阵分解方法类似，聚类数量是一个超参数，需要根据经验预先定义或选择。少量聚类可能导致同一组中出现不同的神经元，而大量聚类可能导致相似的神经元分裂成不同组</li>
</ul>
</li>
<li><p>Multi-model Search</p>
<ul>
<li>为同一任务训练的不同模型，会共享该任务的关键信息。如果一个概念对任务很重要，那么所有为该任务优化的模型都应该学习到它。因此，该搜索的目标是<strong>识别在不同模型间行为相似的神经元</strong>—— 这些神经元很可能编码了任务的核心概念。</li>
<li><p><strong>聚合相关系数</strong>：为了从不同角度凸显模型特征，他们用四种方法对每个神经元的相关系数进行聚合，每种方法目标不同：</p>
<ul>
<li><strong>最大相关（Max Correlation）</strong>：捕捉在多个模型中都 “强烈出现” 的概念（即该神经元与其他模型中某些神经元的相关性极高）。</li>
<li><strong>最小相关（Min Correlation）</strong>：筛选那些与很多模型都有相关性、但并非顶级相关的神经元（强调普遍性而非极致相关性）。</li>
<li><strong>回归排序（Regression Ranking）</strong>：寻找自身信息分散在其他模型多个神经元中的 “个体神经元”（即该神经元的功能由其他模型的多个神经元共同实现）。</li>
<li><strong>SVCCA</strong>：捕捉那些可能分散在 “比完整表示维度更低” 的信息（聚焦低维空间中的关键信息分布）。</li>
</ul>
</li>
<li><p>局限性：即需要人类来解释这些被识别出的神经元的底层含义（比如这个神经元具体编码了什么概念）</p>
</li>
</ul>
</li>
</ul>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><ul>
<li><p>Ablation</p>
<ul>
<li>在模型中的某个神经元前后判断其对输出效果的影响</li>
</ul>
</li>
<li><p>Classification Performance</p>
<ul>
<li>给定与某个概念相关的显著神经元后，评估其正确性的简单方法如下：<ol>
<li><strong>训练分类器</strong>：将这些显著神经元作为特征，训练一个分类器来预测目标概念。</li>
<li><strong>性能对比</strong>：将该分类器的性能，与另外两个分类器进行比较 —— 一个用<strong>随机神经元</strong>训练，另一个用<strong>最不重要的神经元</strong>训练。</li>
<li><strong>衡量有效性</strong>：通过这种相对性能对比，来衡量所选显著神经元的有效性（即它们是否真的与目标概念相关）。</li>
</ol>
</li>
</ul>
</li>
<li>Information Theoretic Metric<ul>
<li>用<strong>信息论指标（如互信息）</strong> 来解读深度学习 NLP 模型表示的思路。</li>
</ul>
</li>
<li>Concept Selectivity<ul>
<li>概念选择性用于衡量<strong>单个神经元</strong>与 “发现的概念（discovered concept）” 之间的 “对齐程度（alignment）”，本质是评估神经元对目标概念的 “专属响应程度”。</li>
<li>判断一个神经元是否 “专门” 对某个语言概念敏感，而不受其他概念干扰。</li>
<li>计算方式<ul>
<li>计算一个神经元在两类句子上的平均激活值：一类是 “包含目标概念的句子”，另一类是 “不包含目标概念的句子”。</li>
<li>用这两个平均激活值的<strong>差值</strong>作为 “选择性” 的量化结果。</li>
</ul>
</li>
</ul>
</li>
<li>Qualitative Evaluation<ul>
<li>将 “可视化” 作为一种非量化（定性）的评估工具，用于分析那些被选中的神经元。它不依赖具体数值，而是通过直观呈现来判断神经元的特性。</li>
<li>可视化单独使用时效果有限，<strong>与其他方法结合才能最大化其价值</strong>，具体体现在 “缩小搜索范围” 上：<ul>
<li>结合<strong>概念搜索（Concept Search）</strong>：可将分析范围缩小到 “对特定概念激活程度高的神经元”，避免无差别分析所有神经元。</li>
<li>结合<strong>基于探针的方法（Probing-based methods）</strong>：能进一步聚焦到 “对这些高激活概念而言，功能最显著的神经元”，让评估更具针对性。</li>
</ul>
</li>
<li>简单来说，可视化负责 “直观展示神经元在关注什么”，而其他方法负责 “告诉我们该看哪些神经元”，二者结合让对神经元的定性评估更高效、准确。</li>
</ul>
</li>
</ul>
<h2 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h2><ul>
<li><p>Concept Discovery</p>
<ul>
<li>神经元可以学习单词在输入句子中的位置</li>
<li>捕捉“否定”的神经元</li>
<li>捕获相关概念组的神经元，如西兰花、面条、胡萝卜</li>
<li>神经元捕获了核心语言概念，如名词、动词形式、数字、冠词等</li>
<li>负责概念的神经元数量因概念的性质而异<ul>
<li>封闭类 (随着语言的发展，不会添加新词) 的神经元数量较少</li>
<li>开放类 (随着语言的发展，不断添加新单词,例如“chillax”，一个混合了“chill”和“relax”的动词。)的神经元数量较多</li>
</ul>
</li>
<li>神经元表现出单义和多义行为其中少数神经元是单一概念所独有的，而其他神经元本质上是多义的，并捕获了多个概念</li>
<li>捕获了远程依赖性包括引号和括号</li>
</ul>
</li>
<li><p>Architectural Analysis</p>
<ul>
<li><p>人类语言的结构是分层的，形态和音韵位于底部，其次是词位，然后是句法结构。发现捕捉单词形态的神经元主要存在于较低层和中间层，而学习句法的神经元则存在于较高层。</p>
</li>
<li><p>学习同形异义词的神经元分布在整个网络中</p>
</li>
<li><p>信息并不是在任何单个层中离散保存的，而是分布的，并且冗余存在于网络中</p>
</li>
<li><p>BERT（自动编码器）中的语言知识在整个网络中高度分布，而 XLNet（自回归）则由来自几层的神经元主要负责一个概念</p>
<p><img src="https://s2.loli.net/2025/10/22/FjsbqfXiCrcDgHZ.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h2 id="神经元探测的应用"><a href="#神经元探测的应用" class="headerlink" title="神经元探测的应用"></a>神经元探测的应用</h2><ul>
<li>控制模型的行为<ul>
<li>在 NMT 模型中确定了开关神经元，这些神经元对现在时动词积极激活，对过去时动词负激活。通过纵这些神经元的值，他们能够在推理过程中成功地将输出翻译从现在时更改为过去时</li>
<li>即时操纵输出，例如，它可用于针对种族和性别等敏感属性消除模型输出的偏差</li>
</ul>
</li>
<li>模型蒸馏和效率<ul>
<li>通过利用Transformer模型中的层和神经元特定冗余，对于模型的提炼和提升效率很有用。</li>
</ul>
</li>
<li>域适应，提高泛化性<ul>
<li>以针对在微调目标域模型时灾难性遗忘一般域的问题。</li>
<li>三步适应过程：<ul>
<li>根据神经元的重要性对神经元进行排名</li>
<li>从网络中修剪不重要的神经元并使用师生框架进行重新训练</li>
<li>将网络扩展到其原始大小并向域内微调，冻结显着神经元并仅调整不重要的神经元。</li>
</ul>
</li>
<li>使用这种方法有助于避免对一般域的灾难性遗忘，同时还可以在域内数据上获得最佳性能。</li>
</ul>
</li>
<li>生成组合解释</li>
</ul>
<h2 id="未来问题和未来研究方向"><a href="#未来问题和未来研究方向" class="headerlink" title="未来问题和未来研究方向"></a>未来问题和未来研究方向</h2><ul>
<li>目前大多数分析方法在寻找与某概念相关的神经元时，<strong>忽略了神经元之间的相互作用</strong>，仅关注单个神经元的贡献。</li>
<li>大量的解释研究依赖于人类定义的语言概念来探测模型。这些模型可能没有严格遵守人类定义的概念并学习有关该语言的新概念。</li>
<li>当前研究多聚焦于 “知识如何编码在模型表征中”，但对 “模型预测时是否真的使用这些编码的知识” 探索较少。</li>
<li>神经元解释的工作缺乏标准的评估基准，因此在相同模型上进行的研究无法比较。</li>
<li>神经元分析方法的理论基础以及它们旨在捕捉的关于给定概念的观点各不相同。这导致神经元的选择可能不会在所有方</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>大模型理解</tag>
        <tag>神经元</tag>
      </tags>
  </entry>
  <entry>
    <title>博客更新流程</title>
    <url>/posts/1108567586/</url>
    <content><![CDATA[<p>​                          <img src="https://s2.loli.net/2025/09/26/BXYQDxA7y8dIeqg.webp" alt="image.png" style="zoom: 15%;" />          </p>
<blockquote>
<p>博客更新流程</p>
</blockquote>
<span id="more"></span>
<ul>
<li>首先执行下面的代码生成新的文章</li>
</ul>
<figure class="highlight actionscript"><table><tr><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">&quot;博客更新流程&quot;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>然后执行下面的代码</li>
</ul>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo <span class="keyword">generate</span></span><br></pre></td></tr></table></figure>
<ul>
<li>然后配置tags以及categories，keywords还有mathjax</li>
</ul>
<figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line"><span class="keyword">tags:</span></span><br><span class="line">  - 教程</span><br><span class="line">categories:</span><br><span class="line">  - 教程</span><br><span class="line">keywords: 博客配置教程</span><br><span class="line">mathjax: true</span><br></pre></td></tr></table></figure>
<ul>
<li>然后添加喜欢的图片，以及简单介绍，还有标识符</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">                          <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://s2.loli.net/2024/11/25/Xhb2w9pEc3dgxOf.png&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;image.png&quot;</span> <span class="attr">style</span>=<span class="string">&quot;zoom: 15%;&quot;</span> /&gt;</span>		  </span><br><span class="line"></span><br><span class="line">&gt; 博客更新流程</span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--more--&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>最后执行</li>
</ul>
<figure class="highlight verilog"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo <span class="keyword">generate</span></span><br><span class="line">hexo deploy</span><br><span class="line">(若要本地预览就在hexo deploy先执行 hexo server)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>消除参考文献中未用的文献</title>
    <url>/posts/717737966/</url>
    <content><![CDATA[<p>​                          <img src="https://s2.loli.net/2025/09/26/KlO3JnGwZbD4kLj.webp" alt="image.png" style="zoom: 15%;" />          </p>
<blockquote>
<p>消除参考文献中未用的文献</p>
</blockquote>
<span id="more"></span>
<ul>
<li><p>首先点击overleaf中里面的输出日志的.aux文件</p>
</li>
<li><p>然后在windows环境下面需要使用Cygwin64 Terminal</p>
</li>
<li><p>然后使用bibtool工具，使用下面代码</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">bibtool -<span class="attribute">x</span> output<span class="selector-class">.aux</span> -<span class="selector-tag">i</span> output<span class="selector-class">.bib</span> -o pruned<span class="selector-class">.bib</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>便可以得到新的精简之后的文件</p>
</li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>喜欢的一些话</title>
    <url>/posts/1527653848/</url>
    <content><![CDATA[<p>​                          <img src="https://s2.loli.net/2024/11/25/Xhb2w9pEc3dgxOf.png" alt="image.png" style="zoom: 15%;" />          </p>
<blockquote>
<p>喜欢的话</p>
</blockquote>
<span id="more"></span>
<ol>
<li>以其不争，故天下莫能与之争</li>
<li>置身事外，谁都可以心平气和；身处其中，谁还可以从容淡定</li>
<li>大音希声  大美无言 大成若缺</li>
<li>因为肩负重担，所以披星戴月；因为心中有梦，所以奋不顾身。</li>
<li>我读得了圣贤书，却管不了这窗外事，心生怜悯是我，袖手旁观也是我，共情是我，无能为力也是我，这情绪像尖刀一样反复刺痛着我的心</li>
<li>我们的心很容易动摇，你得学会哄他，不管发生了什么，告诉你的心“伙计，平安无事”。你也许会问，这样能解决问题吗。答案是不能，但是你得到了面对困难的勇气</li>
<li>每一个问题，都是对你缺点的量身定做，逃避是没有用的，困难是客观的，问题是有解的</li>
<li>对自己的可控的事，永远坚持；对于别人主导的事情，随时放弃；对于无能为力的事情，听天由命</li>
</ol>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
