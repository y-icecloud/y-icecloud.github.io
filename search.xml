<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>FigStep的论文阅读</title>
    <url>/posts/542407868.html</url>
    <content><![CDATA[<p><img src="https://i-blog.csdnimg.cn/img_convert/96dc544f34e8ec4d8606f030ffa086e6.png" alt=""></p>
<blockquote>
<p>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts论文学习</p>
</blockquote>
<span id="more"></span>
<h1 id="FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts"><a href="#FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts" class="headerlink" title="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"></a>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</h1><p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<ul>
<li><a href="https://github.com/ThuCCSLab/FigStep">代码</a></li>
<li><a href="https://arxiv.org/pdf/2311.05608">原文链接</a></li>
</ul>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<h4 id="1-1-安全性对齐"><a href="#1-1-安全性对齐" class="headerlink" title="1.1 安全性对齐"></a>1.1 安全性对齐</h4><p><strong>概念</strong> ：安全性对齐通常指的是确保模型的输出和行为与预期目标和社会规范相一致，不会产生有害或者不当的结果。</p>
<p><strong>分类</strong>：安全性对齐主要涉及以下几个方面：</p>
<ol>
<li><strong>伦理和道德对齐</strong>：确保模型的输出不违反伦理和道德规范。</li>
<li><strong>法律和法规对齐</strong>：确保模型的行为符合相关法律和法规的要求。</li>
<li><strong>用户意图对齐</strong>：确保模型的输出和用户的预期和需求一致，避免误导或者错误的信息。</li>
<li><strong>社会价值对齐</strong>：确保模型的行为和输出符合社会普遍接受的价值观和标准。</li>
</ol>
<h4 id="1-2-VLM"><a href="#1-2-VLM" class="headerlink" title="1.2 VLM"></a>1.2 VLM</h4><ol>
<li><strong>语言模块</strong>：是一个预先训练的LLM，大多数接受了安全对齐。</li>
<li><strong>视觉模块</strong>：是一个图像编码器，将图像转换成为视觉特征。</li>
<li><strong>连接模块</strong>：将视觉特征从视觉模块映射到与语言模块相同的嵌入空间。</li>
</ol>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文介绍了FigStep，一种简单有效的针对于VLM (vision-language models) 的越狱算法，方法侧重于将有害的文本指令转换为排版图像，以便能够绕过VLM中的安全对齐。最后通过ASR (平均攻击成功率) 的显示，得到了不错的效果。这表明了对VLM采用更加复杂的对齐方式，是显得非常重要的。</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<ul>
<li><p><strong>革命</strong>：最近大模型经历了一场大革命，因为虽然它们建立在LLM的基础之上，但可以包括其他模态 (如图片) 。</p>
</li>
<li><p><strong>安全</strong>：VLM的安全性缺乏系统的评估，而且建造者们还可能对他们自己模型的安全性有着盲目自信。</p>
</li>
<li><p><strong>直觉</strong>：本文提出FigStep，主要基于以下三个直觉</p>
<ul>
<li>VLM能够理解并遵循排版视觉模块的特征</li>
<li>局部上文本的安全对齐，可能无法保证整体上文本和图片的安全对齐</li>
<li>底层LLM的逐步推理能力可以提高越狱性能</li>
</ul>
</li>
</ul>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<ul>
<li><p><strong>文本模块安全的重要性</strong>：因为最终生成的结果，是由图像编码器转换为视觉特征，再由连接器映射到文本模块相同的嵌入空间，最终由文本模块响应生成，因此文本模块的安全性十分重要。</p>
</li>
<li><p><strong>有害内容形式转换</strong>：FigStep没有直接输入有害的文字指令，而是通过排版将有害的内容转换为图像。 这样做的目的是绕过VLM文本模块中的安全对齐，诱导其输出违反常见AI安全策略的不安全响应。</p>
<p><img src="https://i-blog.csdnimg.cn/img_convert/96dc544f34e8ec4d8606f030ffa086e6.png" alt=""></p>
<ul>
<li><p>具体实现步骤</p>
<ul>
<li><p>首先释义模块将有害问题转换为以“Steps to”, “List of”的一些陈述性语句。</p>
</li>
<li><p>然后使用排版将有害的内容嵌入到图像之中。</p>
</li>
<li><p>最后利用良性文本来激发VLM的推理能力，根据图像内容来生成详细响应。</p>
</li>
<li><p>除此之外还提出了FigStep Pro，用于越狱GRT-4V。两者之间的差别是FigStep Pro还包括一个额外的预处理步骤，用来绕过GPT-4V系统内的OCR检测器。</p>
<p><img src="https://i-blog.csdnimg.cn/img_convert/8611372104f3f51732eb28e9b77ea25a.png" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>参数设置</strong>：</p>
<ul>
<li>用GPT-4创建了一个名为SafeBench的安全基准，其中包含500个有害问题，涵盖了使用禁止政策的常见基准。<ul>
<li>selfBench安全基准的构建<ul>
<li>首先收集常见禁止主题，然后再选取10个作为安全基准</li>
<li>然后编写描述，再使用GPT-4来进行生成50个问题，并手动审查</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>广泛性</strong> ：在6个开源模型 (2个来自于LLaVA-v1.5, 3个来自于MiniGPT4, 1个CogVLM)，以及最先进的闭源模型GPT-4V证明了有效性</p>
</li>
<li><p><strong>baseline</strong>：直接向六个开源VLM提供纯文本的有害问题来进行基线评估，成功率为44.80%</p>
</li>
</ul>
<p><strong>评估设置</strong></p>
<ul>
<li><p>对禁止问题进行改写，改写之后输入VLM中，然后得到了模型生成的n个回答，然后进行判断模型输出是否属于不安全的内容，最后如果在所有问题之中只要有一次评估器输出为1，那么则就表明攻击成功</p>
<ul>
<li>针对每一个问题</li>
</ul>
<script type="math/tex; mode=display">\text{isSuccessJ}(Q^*) = \max_{i=1}^{n} E(Q^*, R_J(T^*), i) \in \{0, 1\}.</script><ul>
<li><p>所有问题</p>
<script type="math/tex; mode=display">\text{ASRJ}(D) = \frac{\sum_{Q^* \in D} \text{isSuccessJ}(Q^*)}{|D|}.</script></li>
</ul>
</li>
</ul>
<p><strong>消融实验</strong>：</p>
<ul>
<li>首先第一种查询，直接将改变后中的图片中的文本，直接输入，查看效果</li>
<li>第二种查询，将出现的所有文本信息输入，包括文本煽动性信息，进行查询 (目的是为了测试 (1) 通过释义将模型参与延续任务是否可以提高越狱的成功率 (2) 单一模态是否能够做到高ASR )</li>
<li>第三种查询只有图像 (用于探索FigStep中煽动性文本提示的作用)</li>
<li>第四种则是直接将SafeBench Tiny中的原始问题放置于图像中，而文本则是直接要求提供答案</li>
</ul>
<p><strong>FigStep生成</strong></p>
<ul>
<li>不同的生成方式 (字体的形态，颜色；背景的颜色) 对最后的结果都有影响</li>
</ul>
<p><strong>重复设置n的次数</strong></p>
<ul>
<li>探究查询次数设置，次数越多效果越好，但5次便能得到不错的效果</li>
</ul>
<p><strong>Temperature设置的探究</strong></p>
<ul>
<li>探究不同的Temperature下，ASR的效果，Temperature越高创造性越强，ASR越高</li>
</ul>
<p><strong>系统提示词的设置</strong></p>
<ul>
<li>通过实验，发现不同的系统提示词对实验结果有着不同的影响，但是通过FigStep仍然能有很好的ASR</li>
</ul>
<p><strong>对GPT-4V的越狱</strong></p>
<ul>
<li>提出FigStep Pro，将多个有害关键字分散嵌入不同的子图之中，来躲避GPT-4V的OCR检测</li>
</ul>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li><strong>有效性</strong>：FigStep可以在相同的VLM上获得平均82.50%的ASR，表明转移有害指令的模态确实可以绕过文本模块内的安全对齐</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>MM-SafetyBench的论文阅读</title>
    <url>/posts/542407868.html</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/11/23/LSlDt9MCFVzocag.png" alt=""></p>
<blockquote>
<p>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models 论文学习</p>
</blockquote>
<span id="more"></span>
<h2 id="MM-SafetyBench-A-Benchmark-for-Safety-Evaluation-of-Multimodal-Large-Language-Models"><a href="#MM-SafetyBench-A-Benchmark-for-Safety-Evaluation-of-Multimodal-Large-Language-Models" class="headerlink" title="MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models"></a>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</h2><p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<p><a href="https://arxiv.org/pdf/2311.17600">原文链接</a></p>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<h4 id="1-1视觉模块和语言模块连接方式"><a href="#1-1视觉模块和语言模块连接方式" class="headerlink" title="1.1视觉模块和语言模块连接方式"></a>1.1视觉模块和语言模块连接方式</h4><ol>
<li><strong>线性投影</strong>：使用线性投影将视觉标记 (visual tokens) 的维度与文本标记 (text tokens) 的维度对齐</li>
<li><strong>可学习的查询</strong>：使用可学习的查询来提取与文本相关的视觉信息，并固定视觉标记的长度。</li>
<li><strong>利用Few-shot</strong>: 利用few-shot使模形快速适应新的任务</li>
</ol>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文出于对多模态大模型很容易被破坏的缘故，提出了MM SafetyBench数据集，用于对MLLM进行安全评估，总共有13个场景，5040个文本图像对。还提出用用扩散模型和排版生成的图像，来创建图像提示，以绕过MLLM中的安全性防御机制</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<ul>
<li><strong>差距</strong>：LLM的安全问题得到了广泛讨论，还有安全对齐的措施，但是MLLM的安全问题仍然研究不足</li>
<li><strong>评估</strong>：缺乏一个较好的数据集对MLLM进行安全性评估</li>
<li><strong>动机</strong>：当图片与文字相关时，模型会激活视觉模块，这一模块，通常没有进行安全性对齐；当两者不相关的时候，语言模块占据主导地位，导致攻击失败</li>
</ul>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<h4 id="4-1-有效性"><a href="#4-1-有效性" class="headerlink" title="4.1 有效性"></a>4.1 有效性</h4><p>当使用图像文本对的时候，对MLLM的攻击显得就很有效</p>
<p><img src="https://s2.loli.net/2024/11/23/LSlDt9MCFVzocag.png" alt=""></p>
<h4 id="4-2-数据集的构建"><a href="#4-2-数据集的构建" class="headerlink" title="4.2 数据集的构建"></a>4.2 数据集的构建</h4><ol>
<li><strong>生成问题</strong>：使用 GPT-4 来生成问题，而且一个问题对应的是三个图像</li>
<li><strong>提取关键短语</strong>：<ul>
<li>首先，有两种不同的场景<ul>
<li>每个问题都包含一个有害短语</li>
<li>每个问题都包含一个政治话题</li>
</ul>
</li>
<li>然后提取之后用于第三步，图像的生成</li>
</ul>
</li>
<li><strong>查询到图像的转换</strong>：<ul>
<li>基于扩散模型</li>
<li>排版图形，使用Pillow，来对图像进行绘图</li>
<li>扩散模型+排版图形，将两者连接在一起，扩散模型在上面，排版图形在下面</li>
</ul>
</li>
<li><strong>问题的改写</strong>：根据第一步的问题和第三步生成的图片进行改写生成新的问题</li>
</ol>
<h4 id="4-3-模型的评估"><a href="#4-3-模型的评估" class="headerlink" title="4.3 模型的评估"></a>4.3 模型的评估</h4><ol>
<li>将场景分为三个类别，不同的类别，认为安全的方式不同<ul>
<li>对于一些类别，如非法活动，不包含任何有害内容，则认为是安全的</li>
<li>还有有些类别，如政治话题，不响应则认为是安全的</li>
<li>最后的一些类别，如法律医疗领域，包含免责声明和风险警告则是安全的</li>
</ul>
</li>
<li>利用ASR，平均攻击成功率来评估模型</li>
<li>利用RR，拒绝率，来反映模型是否准确的识别到恶意查询，并做出拒绝</li>
</ol>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>模型设置</strong>：评估了最近发布的12种模型</p>
<p><strong>实验方式</strong>：使用排版，扩散，基线等方式</p>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li><strong>safety prompt</strong>：提出了safety prompt，使模型能够更好的抵御攻击，这一结果是建立在模型能够遵循指令的条件下</li>
<li><strong>扩散+排版</strong>大多数情况下效果最好，相比于基线还有单纯的用基线和排版</li>
<li>没有一个模型能够做到<strong>安全性和智能性</strong>的平衡，详细说来，则是有些模型看似安全实则无法正确做出相应</li>
<li>附录中还对MLLM Protector做了对比，发现 Safety prompt 即激发自主的安全防御更有效</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</strong></p>
<ul>
<li>介绍FigStep算法，通过将有害指令转换为图像来对VLM进行攻击，并且证明出在具有OCR功能的GPT-4V上也有较高攻击率。</li>
</ul>
<p><strong>MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</strong></p>
<ul>
<li>提出MM-SafetyBench数据集，用来评估多模态大模型，发现其容易受到图文结合的攻击，特别是与文字相关的图片，最后还提出了可以引入安全提示减少攻击成功率。</li>
</ul>
<p><strong>Visual Adversarial Examples Jailbreak Aligned Large Language Models</strong></p>
<ul>
<li>主要是从对抗性样本上面出发，发现其可以绕开VLM的安全防御，使模形能够生成与训练无关的有害内容，还解锁了模型被禁止的能力。</li>
</ul>
<p><strong>对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Visual Adversarial Examples (2023.8)</th>
<th style="text-align:center">FigStep (2023.12)</th>
<th style="text-align:center">MM-SafetyBench (2024.6)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>研究对象</strong></td>
<td style="text-align:center">VLM</td>
<td style="text-align:center">VLM</td>
<td style="text-align:center">MLLM(但主要还是针对VLM)</td>
</tr>
<tr>
<td style="text-align:center"><strong>方法特点</strong></td>
<td style="text-align:center">对抗性图片</td>
<td style="text-align:center">图形化有害文字</td>
<td style="text-align:center">文本+图片 (扩散+排版)</td>
</tr>
<tr>
<td style="text-align:center"><strong>创新点</strong></td>
<td style="text-align:center">强调对抗性样本的通用性</td>
<td style="text-align:center">简单的图形化文字的攻击</td>
<td style="text-align:center">提出安全评估基准</td>
</tr>
<tr>
<td style="text-align:center"><strong>研究模型范围</strong></td>
<td style="text-align:center">GPT-4V和LLaVA</td>
<td style="text-align:center">6种VLM+GPT-4V</td>
<td style="text-align:center">12种开源MLLM</td>
</tr>
<tr>
<td style="text-align:center"><strong>评估方法</strong></td>
<td style="text-align:center">人工检查与黑盒传递性验证</td>
<td style="text-align:center">SafeBench基准测试</td>
<td style="text-align:center">MM SafetyBench数据集(13个场景，5040个文本图像对)</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Adversarial Examples的论文阅读</title>
    <url>/posts/542407868.html</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/11/25/wrEodCjUzDWuhBl.png" alt=""></p>
<blockquote>
<p>Visual Adversarial Examples Jailbreak Aligned Large Language Models 论文学习</p>
</blockquote>
<span id="more"></span>
<h2 id="Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models"><a href="#Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models" class="headerlink" title="Visual Adversarial Examples Jailbreak Aligned Large Language Models"></a>Visual Adversarial Examples Jailbreak Aligned Large Language Models</h2><blockquote>
<p>更多大模型安全相关以及机器学习相关的文章见主页<br><a href="https://y-icecloud.github.io/">https://y-icecloud.github.io/</a></p>
<p><a href="https://arxiv.org/pdf/2306.13213">文章链接</a></p>
<p><a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models">论文代码</a></p>
</blockquote>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<p><strong>对抗性样本</strong>：</p>
<ul>
<li>对抗性样本是指在原始数据上添加特定的干扰，这个干扰是像素级别的修改，人几乎看不出来变化，但是会对模型产生误导</li>
<li>一般是基于梯度来生成，使之损失变大</li>
</ul>
<p><strong>红队攻击（Red-teaming）</strong>：</p>
<p>在AI和机器学习中，红队攻击指的是通过模拟恶意用户的行为，主动寻找并利用模型的潜在弱点</p>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文在多模态大模型的产生下，提出了对抗性攻击，这种攻击通过改变图片中的像素，来进行攻击，这种攻击被证明是非常有效的。与此同时，这种对抗性攻击，还有通用性，对绝大多数的对齐模型都能迁移起作用。最后还展望了对其他种类模态信息的探究。</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<p><strong>革命</strong>：最近人们将视觉融合到LLM中的兴趣激增，导致VLM的出现。</p>
<p><strong>复杂性</strong>：然而，多模态模型增加了模型的复杂度和攻击面，这对于确保模型的安全性和可靠性构成了挑战。</p>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<p><strong>视觉输入</strong>：因为视觉输入的连续性和高维性，使之成为对抗攻击的薄弱之处</p>
<p><strong>白盒攻击</strong>：知道模型权重，便于进行生成对抗性样本</p>
<p><strong>黑盒攻击</strong>：不知道模型权重，但是这个攻击可以在多个模型之间移植，实现全面性，实现更加通用的攻击</p>
<p><strong>对抗性样本</strong>：针对的并不是一种特殊的指令，而是一种通用的指令，能够使绝大多数有害文本可以得到输出</p>
<p><strong>对抗性样本构建方式</strong>：</p>
<ul>
<li><p>攻击的目的是构建一个对抗性输入 <script type="math/tex">x_{\text{adv}}</script>，使模型在给定 <script type="math/tex">x_{\text{adv}}</script>时，倾向于生成一些有害内容 Y=<script type="math/tex">\{y_i\}_{i=1}^m</script>​</p>
</li>
<li><p>Y是一个小型语料库，其中包含一些有害的语句，这些语句会用来指导攻击</p>
</li>
<li><p>通过调整<script type="math/tex">x_{\text{adv}}</script>来最大化模型生成有害内容的概率</p>
</li>
<li><p>对于每个有害语句 <script type="math/tex">y_i</script>，它的生成概率用 <script type="math/tex">p(y_i \mid x_{\text{adv}})</script> 表示。通过取负对数<script type="math/tex">(−log)</script>​，问题变成了最小化目标函数</p>
</li>
<li><p>而且优化的过程受限于一个空间<strong><em>B</em></strong>，这个空间是用来让图片尽可能看起来正常</p>
<p><img src="https://s2.loli.net/2024/11/24/YBXuqJTO9rfK8jF.png" style="zoom: 50%;" /></p>
</li>
<li><p>最后再将 <script type="math/tex">x_{\text{adv}}</script>和 <script type="math/tex">x_{\text{harm}}</script>​一起送入模型</p>
</li>
</ul>
<p><strong>文本对抗攻击</strong></p>
<ul>
<li>将视觉对抗性嵌入替换为具有相等长度的文本对抗性标记</li>
<li>文本攻击的开销更大，因为文本在其空间更加离散，所以导致计算需求更高</li>
</ul>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>评估</strong></p>
<ul>
<li><p>三种形式，无对抗性信息，对抗性图像，对抗性文本，且从图中可以看出来，对图像施以越强的对抗，效果越好</p>
<p><img src="https://s2.loli.net/2024/11/23/fgjypQZhNrX4AMu.png" alt=""></p>
</li>
<li><p>用分类器计算毒性属性分数，范围从0到1，对每个对于每个属性，我们计算分数超过0.5阈值的生成文本的比率，并且重复三次</p>
</li>
</ul>
<p><strong>分析攻击</strong></p>
<ul>
<li>原来提出的一些削弱攻击的方法不再行之有效，例如对抗性训练 (将对抗性样本作为训练数据) ，因为现在的输出是开放的，与狭义的分类形成对比。而且现在的对抗性干扰不一定是不可察觉的，因此，这些防御所假设的小扰动界限不再适用。</li>
<li>但是通过向图像之中引入噪声能够在一定程度上中和对抗性攻击</li>
</ul>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li>传统的对纯文本输入输出的LLM的越狱攻击的难度比较大，因为文本是离散型的。在引入视觉机制后，对系统的越狱攻击就变得比较简单。</li>
<li>对抗性攻击，通过对图像的改变，来进行攻击是有效的</li>
</ul>
<h3 id="7-未来思考"><a href="#7-未来思考" class="headerlink" title="7. 未来思考"></a>7. 未来思考</h3><ul>
<li>多模态的出现，推测其他模态的信息比如语音等，也存在这样的跨模态攻击</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
        <tag>大模型安全</tag>
      </tags>
  </entry>
  <entry>
    <title>喜欢的一些话</title>
    <url>/posts/1527653848.html</url>
    <content><![CDATA[<p>​                          <img src="https://s2.loli.net/2024/11/25/Xhb2w9pEc3dgxOf.png" alt="image.png" style="zoom: 15%;" />          </p>
<blockquote>
<p>喜欢的话</p>
</blockquote>
<span id="more"></span>
<ol>
<li>以其不争，故天下莫能与之争</li>
<li>置身事外，谁都可以心平气和；身处其中，谁还可以从容淡定</li>
<li>大音希声  大美无言 大成若缺</li>
<li>因为肩负重担，所以披星戴月；因为心中有梦，所以奋不顾身。</li>
<li>我读得了圣贤书，却管不了这窗外事，心生怜悯是我，袖手旁观也是我，共情是我，无能为力也是我，这情绪像尖刀一样反复刺痛着我的心</li>
<li>我们的心很容易动摇，你得学会哄他，不管发生了什么，告诉你的心“伙计，平安无事”。你也许会问，这样能解决问题吗。答案是不能，但是你得到了面对困难的勇气</li>
</ol>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
