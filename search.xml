<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>喜欢的一些话</title>
    <url>/posts/1527653848.html</url>
    <content><![CDATA[<p>​                          <img src="https://s2.loli.net/2024/11/25/Xhb2w9pEc3dgxOf.png" alt="image.png" style="zoom: 15%;" />          </p>
<blockquote>
<p>喜欢的话</p>
</blockquote>
<span id="more"></span>
<ol>
<li>以其不争，故天下莫能与之争</li>
<li>置身事外，谁都可以心平气和；身处其中，谁还可以从容淡定</li>
<li>大音希声  大美无言 大成若缺</li>
<li>因为肩负重担，所以披星戴月；因为心中有梦，所以奋不顾身。</li>
<li>我读得了圣贤书，却管不了这窗外事，心生怜悯是我，袖手旁观也是我，共情是我，无能为力也是我，这情绪像尖刀一样反复刺痛着我的心</li>
</ol>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>Visual Adversarial Examples的论文阅读</title>
    <url>/posts/542407868.html</url>
    <content><![CDATA[<p><img src="https://s2.loli.net/2024/11/25/wrEodCjUzDWuhBl.png" alt=""></p>
<blockquote>
<p>Visual Adversarial Examples Jailbreak Aligned Large Language Models 论文学习</p>
</blockquote>
<span id="more"></span>
<h2 id="Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models"><a href="#Visual-Adversarial-Examples-Jailbreak-Aligned-Large-Language-Models" class="headerlink" title="Visual Adversarial Examples Jailbreak Aligned Large Language Models"></a>Visual Adversarial Examples Jailbreak Aligned Large Language Models</h2><blockquote>
<p><a href="https://arxiv.org/pdf/2306.13213">文章链接</a></p>
<p><a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models">论文代码</a></p>
</blockquote>
<h3 id="1-前置知识"><a href="#1-前置知识" class="headerlink" title="1. 前置知识"></a>1. 前置知识</h3><hr>
<p><strong>对抗性样本</strong>：</p>
<ul>
<li>对抗性样本是指在原始数据上添加特定的干扰，这个干扰是像素级别的修改，人几乎看不出来变化，但是会对模型产生误导</li>
<li>一般是基于梯度来生成，使之损失变大</li>
</ul>
<p><strong>红队攻击（Red-teaming）</strong>：</p>
<p>在AI和机器学习中，红队攻击指的是通过模拟恶意用户的行为，主动寻找并利用模型的潜在弱点</p>
<h3 id="2-论文大体介绍"><a href="#2-论文大体介绍" class="headerlink" title="2. 论文大体介绍"></a>2. 论文大体介绍</h3><hr>
<p>此文在多模态大模型的产生下，提出了对抗性攻击，这种攻击通过改变图片中的像素，来进行攻击，这种攻击被证明是非常有效的。与此同时，这种对抗性攻击，还有通用性，对绝大多数的对齐模型都能迁移起作用。最后还展望了对其他种类模态信息的探究。</p>
<h3 id="3-论文背景"><a href="#3-论文背景" class="headerlink" title="3. 论文背景"></a>3. 论文背景</h3><hr>
<p><strong>革命</strong>：最近人们将视觉融合到LLM中的兴趣激增，导致VLM的出现。</p>
<p><strong>复杂性</strong>：然而，多模态模型增加了模型的复杂度和攻击面，这对于确保模型的安全性和可靠性构成了挑战。</p>
<h3 id="4-方法原理"><a href="#4-方法原理" class="headerlink" title="4. 方法原理"></a>4. 方法原理</h3><hr>
<p><strong>视觉输入</strong>：因为视觉输入的连续性和高维性，使之成为对抗攻击的薄弱之处</p>
<p><strong>白盒攻击</strong>：知道模型权重，便于进行生成对抗性样本</p>
<p><strong>黑盒攻击</strong>：不知道模型权重，但是这个攻击可以在多个模型之间移植，实现全面性，实现更加通用的攻击</p>
<p><strong>对抗性样本</strong>：针对的并不是一种特殊的指令，而是一种通用的指令，能够使绝大多数有害文本可以得到输出</p>
<p><strong>对抗性样本构建方式</strong>：</p>
<ul>
<li><p>攻击的目的是构建一个对抗性输入 <script type="math/tex">x_{\text{adv}}</script>，使模型在给定 <script type="math/tex">x_{\text{adv}}</script>时，倾向于生成一些有害内容 Y=<script type="math/tex">\{y_i\}_{i=1}^m</script>​</p>
</li>
<li><p>Y是一个小型语料库，其中包含一些有害的语句，这些语句会用来指导攻击</p>
</li>
<li><p>通过调整<script type="math/tex">x_{\text{adv}}</script>来最大化模型生成有害内容的概率</p>
</li>
<li><p>对于每个有害语句 <script type="math/tex">y_i</script>，它的生成概率用 <script type="math/tex">p(y_i \mid x_{\text{adv}})</script> 表示。通过取负对数<script type="math/tex">(−log)</script>​，问题变成了最小化目标函数</p>
</li>
<li><p>而且优化的过程受限于一个空间<strong><em>B</em></strong>，这个空间是用来让图片尽可能看起来正常</p>
<p><img src="https://s2.loli.net/2024/11/24/YBXuqJTO9rfK8jF.png" style="zoom: 50%;" /></p>
</li>
<li><p>最后再将 <script type="math/tex">x_{\text{adv}}</script>和 <script type="math/tex">x_{\text{harm}}</script>​一起送入模型</p>
</li>
</ul>
<p><strong>文本对抗攻击</strong></p>
<ul>
<li>将视觉对抗性嵌入替换为具有相等长度的文本对抗性标记</li>
<li>文本攻击的开销更大，因为文本在其空间更加离散，所以导致计算需求更高</li>
</ul>
<h3 id="5-实验设置"><a href="#5-实验设置" class="headerlink" title="5. 实验设置"></a>5. 实验设置</h3><hr>
<p><strong>评估</strong></p>
<ul>
<li><p>三种形式，无对抗性信息，对抗性图像，对抗性文本，且从图中可以看出来，对图像施以越强的对抗，效果越好</p>
<p><img src="https://s2.loli.net/2024/11/23/fgjypQZhNrX4AMu.png" alt=""></p>
</li>
<li><p>用分类器计算毒性属性分数，范围从0到1，对每个对于每个属性，我们计算分数超过0.5阈值的生成文本的比率，并且重复三次</p>
</li>
</ul>
<p><strong>分析攻击</strong></p>
<ul>
<li>原来提出的一些削弱攻击的方法不再行之有效，例如对抗性训练 (将对抗性样本作为训练数据) ，因为现在的输出是开放的，与狭义的分类形成对比。而且现在的对抗性干扰不一定是不可察觉的，因此，这些防御所假设的小扰动界限不再适用。</li>
<li>但是通过向图像之中引入噪声能够在一定程度上中和对抗性攻击</li>
</ul>
<h3 id="6-实验结论"><a href="#6-实验结论" class="headerlink" title="6. 实验结论"></a>6. 实验结论</h3><hr>
<ul>
<li>传统的对纯文本输入输出的LLM的越狱攻击的难度比较大，因为文本是离散型的。在引入视觉机制后，对系统的越狱攻击就变得比较简单。</li>
<li>对抗性攻击，通过对图像的改变，来进行攻击是有效的</li>
</ul>
<h3 id="7-未来思考"><a href="#7-未来思考" class="headerlink" title="7. 未来思考"></a>7. 未来思考</h3><ul>
<li>多模态的出现，推测其他模态的信息比如语音等，也存在这样的跨模态攻击</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>论文阅读</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
</search>
